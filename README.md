# Cloud-Scale Data Engineering project using Azure SQL, ADF CDC, Databricks Auto Loader, Delta Live Tables (SCD-2), and Unity Catalog â€” deployed to Dev & Prod with Databricks Asset Bundles.

End-to-end data engineering pipeline inspired by Spotify data flows, built on **Azure + Databricks** using the **Medallion Architecture**.

Key highlights:
- Incremental ingestion using **Azure Data Factory (CDC)**
- Raw zone stored in **ADLS Bronze**
- **Databricks Auto Loader** starts from Bronze â†’ Silver (not at ingestion)
- **DLT + SCD-2** for Gold business tables
- **Databricks SQL Warehouse** + Power BI serving
- CI/CD using **Databricks Asset Bundles** + GitHub
- **Unity Catalog + Managed Identity** for governance & secure access

---

## ðŸ—ï¸ Architecture Diagram

![Azure Medallion Architecture](./Architecture/azure_medallion_architecture.png)

```text
project-root/
â”œâ”€ Architecture/
â”‚  â””â”€ azure_medallion_architecture.png
â”œâ”€ azure-data-factory/
â”‚  â”œâ”€ pipelines/
â”‚  â”‚  â””â”€ cdc_pipeline.json
â”‚  â”œâ”€ datasets/
â”‚  â””â”€ linked-services/
â”œâ”€ databricks/
â”‚  â”œâ”€ asset-bundle/
â”‚  â””â”€ src/
â”‚     â”œâ”€ silver/
â”‚     â”œâ”€ gold/
â”‚     â”‚  â””â”€ dlt/
â”‚     â””â”€ jinja/
â”œâ”€ utils/
â””â”€ README.md
```
## â˜ï¸ Azure Resource Group & Provisioned Services

All cloud resources for this project are deployed inside a dedicated Azure Resource Group.

### Resource Group: `RG-AzureSpotify`

| Service | Purpose |
|---|---|
Azure SQL Database | Source transactional database  
Azure SQL Server | SQL hosting for source DB  
Azure Data Factory (V2) | CDC pipeline â†’ Bronze  
Azure Databricks Workspace | Medallion ETL + DLT  
Azure Storage Account (ADLS Gen2) | Bronze / Silver / Gold zones  
Access Connector for Databricks | Secure access to ADLS  
Logic App | Alerts & automation  
Office365 Connection | ADF + Logic App notifications  

### Resource Group Screenshot
![Azure Resource Group](./Architecture/AzureResourceGroup.jpeg)



## Azure Data Factory â€” Incremental Pipeline (CDC)

### Pipeline Responsibilities
- Connect to **Azure SQL Source**
- Capture inserts/updates via **CDC**
- Land raw Parquet files into **ADLS Bronze container**
- Track watermark column for incremental loads

### Artifacts

```text
azure-data-factory/
â”œâ”€ pipelines/
â”‚  â””â”€ cdc_pipeline.json
â”œâ”€ datasets/
â”œâ”€ linked-services/
```

### ADF Pipeline Screenshot
```
![ADF CDC Pipeline](./Architecture/ADF UI.jpeg)
```

### ADF Run Output
```
![ADF Run Output](./images/adf_run_output.png)
```

---

## Bronze Layer â€” Raw Data Zone (ADLS Gen2)

### Purpose
Store raw immutable data exactly as received from source systems.

### Characteristics
| Property | Details |
|---|---|
Storage | ADLS Gen2 `/bronze/`
Format | Parquet (raw files)
Ingestion Tool | **Azure Data Factory (CDC)**
Transformation | None (schema-on-read)
Usage | Replay, lineage, auditing, time-travel

### Responsibilities
- Preserve **raw source fidelity**
- Maintain **auditability and traceability**
- Serve as input to **Databricks Auto Loader** in next stage (Silver)

### Folder Structure

```
adls/
â””â”€ bronze/
   â”œâ”€ tableA/
   â”œâ”€ tableB/
   â””â”€ metadata/
```

### Bronze Layer Screenshot
```
![Bronze Storage](./images/bronze_layer.png)
```

---

## Transition â€” Bronze â†’ Silver

- Data moves from **Bronze to Silver**
- Triggered via **Databricks Auto Loader**
- Applied in next section

> Next section: `Silver Layer â€” Cleaned & Refined Data`

## Silver Layer â€” Cleaned & Refined Data (Databricks)

### Purpose
Transform raw Bronze data into clean, structured, and validated Delta tables.

### Characteristics
| Property | Details |
|---|---|
Storage | ADLS Gen2 `/silver/`
Format | Delta Lake
Engine | **Databricks Auto Loader + PySpark**
Schema | Cleaned, validated, conformed schema
Usage | Analytics-ready base tables for Gold layer

### Responsibilities
- Read raw data from **Bronze**
- Apply **schema enforcement**
- Handle **missing / bad records**
- Normalize and cast datatypes
- Deduplicate using watermark logic
- Write optimized **Delta tables**

### Processing Flow

```
Bronze (Raw Parquet/Delta)
     â†“ Auto Loader (streaming/batch)
PySpark cleansing + transformations
     â†“
Silver Delta Tables
```

### Folder Structure

```
adls/
â””â”€ silver/
   â”œâ”€ cleaned_tableA/
   â”œâ”€ cleaned_tableB/
   â””â”€ checkpoints/
```

### Notebook / Code Sample

```python
df = (spark.readStream.format("cloudFiles")
      .option("cloudFiles.format", "parquet")
      .load("/mnt/bronze/tableA"))

df_clean = (df
    .dropDuplicates(["id"])
    .filter("id IS NOT NULL")
    .withColumn("event_ts", col("event_ts").cast("timestamp"))
)

df_clean.writeStream \
    .format("delta") \
    .option("checkpointLocation", "/mnt/silver/checkpoints/tableA") \
    .start("/mnt/silver/cleaned_tableA")
```

### Auto Loader Config Screenshot
```
![Silver AutoLoader](./images/silver_autoloader.png)
```

### Silver Tables Screenshot
```
![Silver Delta Tables](./images/silver_tables.png)
```

---

## Transition â€” Silver â†’ Gold

- Silver tables feed **Delta Live Tables (DLT)** pipelines
- Business rules & aggregations applied next

> Next section: `Gold Layer â€” Business & Analytics Layer (Delta Live Tables)`


## ðŸŸ¡ Gold Layer â€” Delta Live Tables + Jinja SQL Automation

The Gold layer uses **Delta Live Tables (DLT)** to build business-ready dimension and fact tables, with:

- **SCD Type-2** for dimensions using Databricks Auto-CDC
- **Upserts** for fact tables (Type-1)
- **Jinja templating** inside the DLT notebook to dynamically generate SQL
- **Databricks Asset Bundles** for Dev â†’ Prod deployment

### Key Capabilities

| Feature | Implementation |
|---|---|
SCD Type-2 Dimensions | DLT `@dlt.create_streaming_table` + Auto-CDC  
Fact Upserts | DLT + merge logic  
SQL Generator | Jinja templated SQL strings  
Processing | Spark structured streaming  
Deployment | Databricks Asset Bundles â†’ Dev & Prod  
Scheduler | Databricks Jobs pipeline  

### Architecture Flow (Gold)

```
DLT Pipeline â†’ Jinja SQL â†’ Delta Tables (Gold Zone) â†’ Dev & Prod via Asset Bundle
```

### ðŸ“¸ DLT Pipeline Screenshot  
`![DLT Pipeline](./images/dlt_pipeline.png)`

### ðŸ“¸ Jinja templating inside DLT notebook  
`![Jinja Gold Notebook](./images/jinja_gold_notebook.png)`

### ðŸ“¸ Auto-CDC + SCD2 lineage in UI  
`![DLT SCD2 View](./images/dlt_scd2_view.png)`

### ðŸ“¸ Databricks Job Triggering Gold Pipeline  
`![DLT Job Run](./images/databricks_job_gold.png)`

### ðŸ“¸ Asset Bundle Deployment â€” Dev & Prod  
`![Asset Bundle Deploy](./images/asset_bundle_deploy.png)`

### Output Tables Example  
`![Gold Tables](./images/gold_tables_catalog.png)`

---

### Summary

| Layer | Engine | Logic |
|---|---|---|
Bronze | ADF | CDC â†’ ADLS |
Silver | Databricks | Auto Loader + PySpark |
**Gold** | **Databricks DLT** | **Auto-CDC + SCD2 + Jinja SQL** |
Deployment | Databricks Asset Bundles | Dev & Prod |

âœ” Production-grade DLT pipeline  
âœ” Real SCD-2 + incremental facts  
âœ” SQL automation via Jinja  
âœ” CI/CD with Databricks Bundle  

---




